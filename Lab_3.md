# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил(а):
- Трофимова Ольга Сергеевна
- РИ210930
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | # | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

## Цель работы
познакомиться с программными средствами для создания
системы машинного обучения и ее интеграции в Unity.

## Постановка задачи
В данной лабораторной работе мы создадим ML-агент и будем тренировать
нейросеть, задача которой будет заключаться в управлении шаром. Задача шара
заключается в том, чтобы оставаясь на плоскости находить кубик, смещающийся в
заданном случайном диапазоне координат.

## Задание 1
### Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity.
При выполнении задания можно использовать видео-
материалы и исходные данные, предоставленные преподавателями курса.
Создайте новый пустой 3D проект на Unity.
Скачайте папку с ML агентом. Вы найдете ее в облаке с исходными
файлами к лабораторной работе – ml-agents-release_19.
В созданный проект добавьте ML Agent, выбрав Window - Package
Manager - Add Package from disk. Последовательно добавьте .json –
 -  ml-agents-release_19 / com,unity.ml-agents / package.json
 -  ml-agents-release_19 / com,unity.ml-agents.extensions / package.json
Если все сделано правильно, то во вкладке с компонентами
(Components) внутри Unity вы увидите строку ML Agent.
Далее запускаем Anaconda Prompt для возможности запуска команд
через консоль.
Далее пишем серию команд для создания и активации нового ML-
агента, а также для скачивания необходимых библиотек:
 -  mlagents 0.28.0;
 -  torch 1.7.1;
 -  Создайте на сцене плоскость, куб и сферу так, как показано на рисунке
ниже. Создайте простой C# скрипт-файл и подключите его к сфере.
 - В скрипт-файл RollerAgent.cs добавьте код, опубликованный в
материалах лабораторных работ – по ссылке.
Объекту «сфера» добавить компоненты Rigidbody, Decision Requester,
Behavior Parameters и настройте их так, как показано на рисунке ниже.
 - В корень проекта добавьте файл конфигурации нейронной сети,
доступный в папке с файлами проекта по ссылке.
 - Pапустите работу ml-агента.
 - Вернитесь в проект Unity, запустите сцену, проверьте работу ML-
Agent’a.
 - Сделайте 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустите
симуляцию сцены и наблюдайте за результатом обучения модели.
 - После завершения обучения проверьте работу модели.
 - Сделайте выводы.

Создан проект в Unity:
![image](https://user-images.githubusercontent.com/103726508/201183065-17c67fcf-14a0-45b2-8c6a-af599bff7606.png)
Написан код:
![image](https://user-images.githubusercontent.com/103726508/201184576-5d62cdb0-540b-477d-ba66-439811ce3ad7.png)
![image](https://user-images.githubusercontent.com/103726508/201184598-a7e28295-1b24-406c-86ab-6be24a48bff1.png)






## Задание 2

Подробно опишите каждую строку файла конфигурации
нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно
найдите информацию о компонентах Decision Requester, Behavior Parameters,
добавленных на сфере.
![image](https://user-images.githubusercontent.com/103726508/204102148-f4be934f-bfcd-42ac-920d-d1ba301981e3.png)


behaviors: описание поведения объекта

RollerBall: имя объекта

trainer_type: тип используемого для обучения тренажёра, здесь обучение с поощрением

batch_size: количество опытов на каждой итерации градиентного спуска

buffer_size: количество опыта, которое нужно собрать для обновления итерации или её изучения

learning_rate: изменение скорости обучения модели с течением времени

beta: сила регуляризации энтропии, которая делает политику "более случайной". Это гарантирует, что агенты должным образом исследуют пространство действий во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий

epsilon: параметр влияющий на быстроту развития (ускорения работы) системы с каждой итерацией

lambd: параметр оценивающий совпадение стоимости вознаграждения обучений между собой (что то вроде погрешности), чем стабильнее значения, тем быстрее идёт процесс.

num_epoch: количество проходов через буфер опыта при выполнении оптимизации градиентного спуска (чем больше, тем быстрее итерация и наоборот)

learning_rate_schedule: скорость обучения, которая в нашем случае уменьшается линейно до нуля.

normalize: отвечает за то будут ли нормироваться (усредняться) входные данные 

hidden_units: количество значений в подключенном слое нейронной сети (то есть чем больше будет поток входных данных, тем больше нужно ввести значения параметра и наоборот).

num_layers: количество скрытых слоев в нейронной сети(число слоёв, принимающих на себя работу, чем сложнее задача, тем больше нужно слоёв)

gamma: параметр, отвечающий за то, насколько далеко вперёд должен думать о вознаграждении агент. Должен быть в состоянии подготовиться – выделить объём места и т.д.

strength: коэффициент, на который умножается вознаграждение, предоставляемое средой. Благодаря этому параметру можно увеличивать или уменьшать количество поступаемой валюты

max_steps: общее количество шагов-действий, которые должны быть выполнены в среде до завершения процесса обучения

time_horizon: сколько опыта нужно собрать перед тем, как добавить в буфер. Также используется, как среднее значение для общего ожидаемого вознаграждения

summary_freq: количество опыта, которое необходимо собрать перед созданием и отображением статистики обучения

hyperparameters: группировка параметров, отвечающих за управления процессом обучения

network_settings: группировка параметров, отвечающих за обучение сети

reward_signals: раздел позволяющий задавать настройки как для внешних, так и для внутренних сигналов вознаграждения

extrinsic: внешний сигнал из reward_signals

Decision Requester это компонент автоматически запрашивающий решение с постоянным интервалом времени. Тоесть он отвечает за принятие решения в цикле: наблюдение-принятия решения-действие-вознаграждение.

Behavior Parameters -это компонент выполняющий функции настройки поведения агента (генерирует объекты и их свойства согласно заданным параметрам).

## Задание 3
Доработайте сцену и обучите ML-Agent таким образом, чтобы шар
перемещался между двумя кубами разного цвета. Кубы должны, как и в первом
задании, случайно изменять координаты на плоскости.

## Выводы

В ходе Лабораторной работы №3 я познакомилась с программными средствами для создания
системы машинного обучения и ее интеграции в Unity, создала проект, обучающий MLAgent.
